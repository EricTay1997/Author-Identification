{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get list of authors\n",
    "train_directory = './C50/C50train'\n",
    "test_directory = './C50/C50test'\n",
    "authors = set()\n",
    "for filename in os.listdir(train_directory):\n",
    "    authors.add(filename)\n",
    "authors.remove(\".DS_Store\")\n",
    "authors = sorted(list(authors))\n",
    "print(len(authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# train and test are dictionaries\n",
    "# keys are the 10 authors\n",
    "# values are the 50 texts written by the author\n",
    "train = {}\n",
    "test = {}\n",
    "for author in authors:\n",
    "    train[author] = []\n",
    "    test[author] = []\n",
    "for author in authors:\n",
    "    for filename in os.listdir(train_directory + \"/\" + author):\n",
    "        f = open(train_directory + \"/\" + author + \"/\" + filename, \"r\")\n",
    "        train[author].append(f.read())\n",
    "        f.close()\n",
    "    for filename in os.listdir(test_directory + \"/\" + author):\n",
    "        f = open(test_directory + \"/\" + author + \"/\" + filename, \"r\")\n",
    "        test[author].append(f.read())\n",
    "        f.close()\n",
    "print(len(train[authors[0]]))\n",
    "print(len(test[authors[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize data\n",
    "for author in authors:\n",
    "    train[author].extend(test[author][:40])\n",
    "    test[author] = test[author][40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/erictay1997/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/erictay1997/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenise(text):\n",
    "    ret = []\n",
    "    sentences = splitter.tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        token_list = tokenizer.tokenize(sentence)\n",
    "        for token in token_list:\n",
    "            ret.append(token.lower())\n",
    "    return ret\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def process_text(text):\n",
    "    token_list = tokenise(text)\n",
    "    # Pos Tag\n",
    "    pos = nltk.pos_tag(token_list)\n",
    "    # Lemmatize\n",
    "    return [lemmatizer.lemmatize(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_2(text):\n",
    "    sentences = splitter.tokenize(text)\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    for tokenized_sentence in tokenized_sentences:\n",
    "        for i in range(len(tokenized_sentence)):\n",
    "            tokenized_sentence[i] = tokenized_sentence[i].lower()\n",
    "    return tokenized_sentences\n",
    "\n",
    "a = tokenise_2(\"i Am hEre. no you\")\n",
    "\n",
    "def process_text_2(text):\n",
    "    list_of_token_lists = tokenise_2(text)\n",
    "    for i in range(len(list_of_token_lists)):\n",
    "        token_list = list_of_token_lists[i]\n",
    "        # Pos Tag\n",
    "        pos = nltk.pos_tag(token_list)\n",
    "        # Lemmatize\n",
    "        list_of_token_lists[i] = [lemmatizer.lemmatize(word,get_wordnet_pos(pos_tag)) for (word,pos_tag) in pos]\n",
    "    return list_of_token_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "train_processed = {}\n",
    "test_processed = {}\n",
    "for i in range(len(authors)):\n",
    "    author = authors[i]\n",
    "    print(i)\n",
    "    train_processed[author] = [process_text(text) for text in train[author]]\n",
    "    test_processed[author] = [process_text(text) for text in test[author]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# train and test are dictionaries\n",
    "# keys are the 50 authors\n",
    "# values are 50 token lists written by the author\n",
    "train_processed_2 = {}\n",
    "test_processed_2 = {}\n",
    "for i in range(len(authors)):\n",
    "    author = authors[i]\n",
    "    print(i)\n",
    "    train_processed_2[author] = [process_text_2(text) for text in train[author]]\n",
    "    test_processed_2[author] = [process_text_2(text) for text in test[author]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "author_dicts = {}\n",
    "vocab = set()\n",
    "for author in authors:\n",
    "    author_dicts[author] = Counter()\n",
    "    for document in train_processed_2[author]:\n",
    "        for sentence in document:\n",
    "            author_dicts[author] += Counter(sentence)\n",
    "    vocab.update(author_dicts[author].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "for word in vocab:\n",
    "    counter = 0\n",
    "    for author in authors:\n",
    "        if word in author_dicts[author]:\n",
    "            counter += 1\n",
    "    word_counts[word] = counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idf = {}\n",
    "for word in vocab:\n",
    "    idf[word] = np.log(len(authors)/word_counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2559e-01,  1.3630e-02,  1.0306e-01, -1.0123e-01,  9.8128e-02,\n",
       "        1.3627e-01, -1.0721e-01,  2.3697e-01,  3.2870e-01, -1.6785e+00,\n",
       "        2.2393e-01,  1.2409e-01, -8.6708e-02,  3.3010e-01,  3.4375e-01,\n",
       "       -8.7582e-04, -2.9658e-01,  2.4417e-01, -1.1592e-01, -3.5742e-02,\n",
       "       -1.0830e-02,  2.0776e-01,  2.9285e-01, -7.3491e-02, -1.8598e-01,\n",
       "       -2.0090e-01, -9.5366e-02,  6.3732e-03, -1.3620e-01,  9.2028e-02,\n",
       "       -3.9957e-02,  1.9027e-01, -1.0456e-01,  2.7670e-03, -7.1742e-01,\n",
       "       -1.2915e-01, -1.3451e-03,  2.7002e-01, -5.3023e-02,  2.2148e-01,\n",
       "        1.3881e-01, -1.5051e-01, -1.9150e-01,  1.6402e-01,  9.7484e-02,\n",
       "        5.6841e-02,  3.9789e-01,  4.0725e-01,  1.4802e-01,  2.1569e-01,\n",
       "       -1.0671e-01, -1.0232e-01,  2.4810e-02, -2.2100e-01, -1.0720e-02,\n",
       "        1.4234e-01, -2.8242e-01,  1.9254e-01,  8.6720e-02, -3.8970e-01,\n",
       "        1.1321e-01,  1.3779e-03,  6.4009e-03, -1.6206e-01, -8.2153e-02,\n",
       "       -5.5397e-01,  3.6789e-01, -4.0159e-03,  2.0710e-01, -3.7157e-01,\n",
       "        2.5135e-01, -1.9544e-01, -4.7059e-02,  1.7155e-01, -2.4036e-01,\n",
       "       -4.6086e-02,  1.9429e-01, -1.8939e-01, -7.1974e-03,  6.9481e-02,\n",
       "        5.9175e-02, -1.7585e-01,  1.0653e-01,  1.6933e-01, -3.6122e-02,\n",
       "        2.9911e-02, -1.1830e-01,  1.3916e-01, -3.7951e-02,  1.0690e-01,\n",
       "       -2.6069e-01, -1.0307e-01, -1.2272e-01, -1.5032e-01, -4.2409e-02,\n",
       "        1.3354e-02, -2.8510e-01,  1.1248e-02,  1.6073e-01, -1.6384e-01,\n",
       "        2.1233e-01, -1.8476e-01, -9.0874e-04,  6.6687e-02,  1.6918e-01,\n",
       "       -3.5004e-01,  9.9016e-02,  4.6393e-01, -1.9462e-01,  1.0346e-01,\n",
       "       -2.5668e-01, -3.6516e-01, -1.8963e-01, -2.1933e-01,  2.4634e-02,\n",
       "        6.5627e-02, -1.1120e-01, -1.6400e-01,  1.0874e-02, -8.4688e-02,\n",
       "       -1.4923e-01, -7.0223e-02,  2.8887e-02,  8.3497e-02, -1.6193e-02,\n",
       "       -2.4926e-03,  1.7186e-01,  9.8749e-03,  8.0237e-02,  1.4774e-01,\n",
       "        4.3206e-02,  2.7716e-01,  5.7697e-01, -4.1297e-02,  1.2765e-01,\n",
       "       -9.1517e-02,  1.4132e-01,  8.7579e-02,  9.3224e-02,  1.5346e-02,\n",
       "       -1.9856e-01,  1.7277e-02, -1.0708e-01, -1.3059e-02, -3.7227e-01,\n",
       "        7.8568e-02,  1.6677e-01, -1.5359e-01, -3.3294e-01,  3.6986e-02,\n",
       "        1.1697e-01,  3.9781e-02,  3.8464e-02, -1.6247e-01,  4.1280e-01,\n",
       "       -7.7491e-02,  4.5490e-02,  1.1330e-01,  8.2177e-03, -2.5052e-01,\n",
       "        7.0966e-02, -1.1388e-01, -1.1503e-01, -1.1014e-01,  1.0499e-01,\n",
       "        1.5878e-01, -2.7023e-01, -1.1006e-02,  7.6057e-04,  3.3902e-01,\n",
       "        2.5564e-01,  1.6342e-01, -5.6019e-01,  1.3055e-01,  7.6311e-02,\n",
       "       -2.8334e-02,  2.8721e-01, -2.7844e-02, -1.1561e-01,  3.4925e-01,\n",
       "       -1.2420e-01,  2.1405e-01,  2.4116e-01, -3.1343e-02,  1.0913e-01,\n",
       "       -2.4755e-01, -4.5429e-02, -8.2178e-02, -1.8831e-01,  1.8446e-01,\n",
       "       -9.7074e-02,  3.2395e-01,  1.0658e-01, -2.6676e-01, -2.7311e-01,\n",
       "        1.7181e-02,  2.5796e-01, -2.8048e-01,  3.0790e-01, -2.1800e-01,\n",
       "        8.7415e-01, -1.2297e-01,  1.0991e-01, -2.9797e-01,  1.3394e-01,\n",
       "        1.0615e-01, -1.0789e-01, -3.5976e-01, -1.8311e-01, -4.5133e-01,\n",
       "        3.4967e-02, -1.9847e-01,  2.1965e-01,  8.1520e-02,  2.5810e-01,\n",
       "        4.0173e-02,  3.1394e-02,  1.9069e-01,  7.5800e-02, -6.0638e-02,\n",
       "        2.0739e-01,  9.8390e-03, -2.6930e-01,  6.6515e-02, -1.0711e-01,\n",
       "        5.9916e-03,  2.3284e-01, -5.8663e-02,  9.8993e-02, -8.1464e-02,\n",
       "        6.7004e-02, -1.4305e-01,  2.5506e-01, -3.1971e-01, -3.1070e-02,\n",
       "       -9.2451e-02,  2.9440e-01,  2.8947e-01, -5.9804e-02,  2.4286e-01,\n",
       "       -1.6755e-01,  4.2031e-02,  5.1261e-01,  2.4525e-01, -6.5983e-01,\n",
       "        6.2456e-02,  5.2204e-02, -2.5717e-02, -8.0613e-02,  8.0869e-02,\n",
       "        2.2821e-01, -1.0217e-01, -2.0719e-01, -1.2123e-02,  3.4916e-01,\n",
       "        8.6527e-02,  6.6288e-02, -9.9828e-02,  2.5843e-01,  1.1943e-01,\n",
       "       -1.3667e-01, -4.3962e-01,  2.3704e-01,  3.1296e-02,  7.4701e-02,\n",
       "       -2.2387e-01,  7.8162e-03, -1.9016e-01,  4.4444e-02,  2.0191e-01,\n",
       "       -2.0814e-01, -2.8382e-01,  1.0427e-01, -2.1098e-01,  1.8865e-01,\n",
       "        3.1659e-01, -2.0753e+00, -7.1045e-02,  5.2419e-01,  5.6023e-02,\n",
       "       -2.5295e-01, -6.2168e-02, -1.0989e-01, -3.5755e-01, -7.9244e-02,\n",
       "        3.7472e-01, -2.8353e-01,  1.6337e-01,  1.1165e-01, -9.8002e-02,\n",
       "        6.0148e-02, -1.5619e-01, -1.1949e-01,  2.3445e-01,  8.1367e-02,\n",
       "        2.4618e-01, -1.5242e-01, -3.4224e-01, -2.2394e-02,  1.3684e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "# glove_vectors = gensim.downloader.load('glove-twitter-50')\n",
    "# or \n",
    "# this performs better\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "glove_vectors[\".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_averaged = {}\n",
    "for author in authors:\n",
    "    w2v_averaged[author] = []\n",
    "    for document in train_processed_2[author]:\n",
    "        w2v_document_averaged = []\n",
    "        for sentence in document:\n",
    "            w2v_sentence = [glove_vectors[word] for word in sentence if word in glove_vectors]\n",
    "            if len(w2v_sentence) == 0:\n",
    "                continue\n",
    "            weights = [idf[word] for word in sentence if word in glove_vectors]\n",
    "            if sum(weights) != 0:\n",
    "                w2v_sentence_averaged = np.zeros(len(w2v_sentence[0]))\n",
    "                for i in range(len(weights)):\n",
    "                    w2v_sentence_averaged += w2v_sentence[i]*weights[i]\n",
    "                w2v_sentence_averaged /= sum(weights)\n",
    "                w2v_sentence_averaged = w2v_sentence_averaged.astype(np.float32)\n",
    "            else:\n",
    "                w2v_sentence_averaged = np.mean(np.array(w2v_sentence), axis=0)\n",
    "            w2v_document_averaged.append(w2v_sentence_averaged)\n",
    "        w2v_averaged[author].append(w2v_document_averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8273e-01,  2.2682e-01, -1.2728e-01,  6.6909e-02, -1.4604e-01,\n",
       "          -3.7018e-01, -7.4869e-02,  1.9472e-01,  1.5710e-01, -1.2026e+00,\n",
       "          -1.0626e-01,  1.9471e-01, -7.0041e-02, -1.2315e-01,  1.5450e-02,\n",
       "           2.2973e-03,  1.0530e-01,  1.1025e-01, -4.4617e-01, -3.8690e-02,\n",
       "           1.9775e-01, -6.3580e-02, -1.4941e-02, -1.4927e-01, -3.6061e-01,\n",
       "           2.2657e-01,  1.1178e-02,  6.7852e-02, -2.5396e-01,  1.8218e-01,\n",
       "          -1.9957e-01, -6.6664e-02,  1.8566e-01, -1.3137e-02, -3.9887e-01,\n",
       "           4.1236e-02, -1.7729e-01, -3.4348e-01, -6.5909e-02, -1.0181e-02,\n",
       "           6.7758e-02, -8.6587e-04, -1.9311e-02,  2.0790e-01, -1.2075e-01,\n",
       "          -6.7033e-02, -1.1569e-01, -1.9777e-01,  3.7347e-02, -6.0328e-02,\n",
       "          -4.0233e-03, -2.1942e-01,  2.5179e-01, -9.9700e-02, -7.2769e-02,\n",
       "          -6.2959e-02, -4.5002e-01, -1.4022e-02, -1.0687e-01,  2.0753e-01,\n",
       "           2.4651e-01,  8.4799e-03, -2.9299e-01, -5.2405e-03, -2.2577e-02,\n",
       "          -5.8664e-02, -1.6939e-01,  3.0973e-02,  1.9886e-01,  8.0195e-02,\n",
       "           3.1717e-01,  8.9696e-02,  4.0284e-01,  2.5108e-01,  1.0020e-01,\n",
       "          -5.2432e-02,  1.1170e-03,  2.0775e-01, -8.3159e-02, -1.9296e-01,\n",
       "          -1.8504e-01,  8.8469e-03, -4.0802e-03, -1.4465e-01,  1.9517e-01,\n",
       "           9.3569e-02,  6.9291e-03,  1.6330e-02, -2.2100e-01,  2.6639e-02,\n",
       "          -3.1809e-01,  3.1596e-02,  1.1145e-01,  1.6670e-02, -8.9086e-02,\n",
       "          -1.2577e-01, -1.6938e-01, -1.5783e-02,  1.6913e-01, -2.5515e-01,\n",
       "          -2.3781e-01, -6.2747e-02, -1.0861e-01,  5.9068e-02, -1.0651e-01,\n",
       "           9.2787e-02, -1.9899e-01, -1.2792e-01, -3.4920e-02, -6.9958e-02,\n",
       "          -1.3002e-01,  7.8769e-02, -3.8676e-01,  1.3193e-01,  1.8847e-01,\n",
       "           5.6310e-02,  2.1206e-01,  2.3216e-01, -2.4369e-01, -1.0184e-01,\n",
       "           1.2744e-01,  7.1469e-02,  2.8152e-01, -2.2494e-01, -1.5739e-01,\n",
       "          -1.4186e-01,  2.3083e-01,  3.1529e-01,  4.6507e-02,  5.1806e-02,\n",
       "           3.0894e-01,  7.2928e-02, -1.9227e-02, -1.0579e-01, -6.7760e-02,\n",
       "          -8.7975e-02, -2.1477e-01,  5.4110e-02,  2.9155e-02,  9.4326e-02,\n",
       "           1.9889e-01, -1.3589e-01,  2.2924e-01, -2.5168e-02, -5.4372e-03,\n",
       "           1.9502e-01,  1.1957e-01, -1.6006e-01,  8.3341e-02,  4.1172e-02,\n",
       "           1.9996e-02, -1.1111e-01,  6.6487e-02, -1.6707e-01,  4.3438e-02,\n",
       "           3.8789e-02,  1.7295e-01, -1.0194e-02,  5.5190e-02, -1.7863e-01,\n",
       "           2.4557e-01, -2.6842e-01, -3.8974e-02,  5.2137e-02,  1.8457e-01,\n",
       "          -5.4007e-02,  1.1056e-01,  3.6366e-01, -7.4944e-02,  1.3292e-01,\n",
       "          -4.6496e-02,  1.3987e-02, -2.9498e-01, -1.3880e-01, -1.7010e-01,\n",
       "          -2.6537e-01,  2.2349e-01, -6.8904e-02,  1.6784e-02, -1.3595e-01,\n",
       "          -2.3960e-01, -3.2830e-01, -1.5539e-02,  1.5625e-01, -2.5035e-01,\n",
       "           2.2749e-01,  1.9093e-01, -1.3626e-01,  5.0975e-02, -4.2936e-02,\n",
       "           7.3182e-02,  3.7523e-02, -3.5300e-01,  1.4521e-01,  2.5398e-01,\n",
       "          -1.3264e-01,  1.4707e-01,  7.3277e-02, -5.1707e-02, -9.3970e-02,\n",
       "          -1.5633e-01,  1.6880e-01,  3.9417e-01, -4.9474e-03,  6.7972e-02,\n",
       "           4.5485e-01, -3.2649e-01,  1.2189e-01, -7.4375e-02,  3.8627e-01,\n",
       "           1.2039e-01, -9.5394e-02, -1.1180e-01,  1.6689e-01,  4.4085e-02,\n",
       "          -1.2300e-01,  1.5438e-01, -1.5048e-02, -4.8245e-02,  5.7009e-02,\n",
       "          -2.8467e-01,  6.4088e-03,  1.6250e-01,  1.1994e-01,  1.2707e-02,\n",
       "          -1.8891e-01,  6.4586e-03,  7.0728e-02,  1.5309e-01, -2.9164e-01,\n",
       "           1.1719e-01,  4.0215e-01,  7.2332e-03, -8.5765e-02, -9.7690e-02,\n",
       "          -2.3784e-01, -2.9015e-02, -1.2136e-01, -1.9065e-01,  1.5473e-01,\n",
       "           1.2754e-01,  3.2660e-01,  1.4260e-01, -6.1173e-02, -6.4500e-01,\n",
       "           1.7651e-01,  4.3305e-02, -3.4452e-01,  1.1686e-01, -2.4419e-01,\n",
       "           1.6744e-02, -7.4746e-02,  1.5351e-01, -3.4959e-01,  5.3275e-02,\n",
       "           1.1644e-02, -1.3822e-01,  6.7700e-02, -2.3454e-02, -1.8372e-02,\n",
       "           6.3605e-02, -2.5637e-01,  4.7075e-02,  2.3225e-01, -9.6655e-02,\n",
       "           1.1156e-01, -1.3283e-01, -2.8862e-01,  3.5567e-02, -2.3006e-01,\n",
       "           2.1163e-01,  5.8942e-02,  8.9003e-02, -3.6527e-02, -2.3931e-01,\n",
       "          -1.7101e-01, -1.1459e+00,  1.1764e-01,  3.5751e-01,  5.2841e-02,\n",
       "          -1.1750e-01,  6.7078e-02, -5.8508e-02,  2.5690e-01,  6.5154e-02,\n",
       "          -1.5118e-01,  7.3951e-02, -1.8070e-02, -1.9369e-01,  1.3056e-02,\n",
       "          -2.0649e-03,  1.6936e-01,  1.1347e-02, -4.9598e-02, -1.1606e-01,\n",
       "          -1.8222e-01, -8.0211e-02,  1.9732e-01,  1.8119e-01, -2.2123e-01]],\n",
       "\n",
       "        [[ 2.5532e-01,  1.7987e-02,  1.3187e-02, -1.6567e-01, -1.7297e-01,\n",
       "          -1.6575e-01, -1.1157e-01,  2.3692e-02, -4.2736e-02, -2.4511e-01,\n",
       "          -1.4124e-02,  2.3747e-01, -6.1435e-03, -2.1784e-02,  3.9311e-01,\n",
       "           4.7661e-01,  1.3648e-01,  2.9783e-02, -1.8645e-01,  1.7331e-02,\n",
       "           5.7686e-02,  4.8518e-01, -2.5055e-01, -2.2294e-01,  2.9642e-02,\n",
       "           2.7886e-01,  2.7361e-01,  7.3838e-02, -2.6958e-01, -4.7071e-02,\n",
       "          -6.5183e-02, -1.7896e-02,  3.2988e-01,  4.6104e-01, -4.6501e-01,\n",
       "           1.6919e-02,  1.6464e-01, -2.9853e-01, -2.6953e-02, -1.4002e-01,\n",
       "           1.6521e-01,  6.6450e-02, -8.0360e-02,  1.7204e-01, -5.2660e-02,\n",
       "          -8.2026e-02, -2.5926e-01, -7.2660e-02, -1.7035e-01, -1.1594e-01,\n",
       "          -5.5006e-02,  3.0867e-01,  1.9574e-01,  5.0390e-02, -7.4536e-02,\n",
       "           2.6080e-01, -2.5722e-01,  3.6961e-01,  7.0921e-02, -8.1589e-02,\n",
       "           3.8805e-02,  1.2416e-01, -1.7985e-01, -2.0784e-01,  1.8059e-02,\n",
       "           5.3152e-02, -6.4084e-02,  3.8870e-01,  1.0615e-01, -3.0004e-01,\n",
       "          -1.5184e-01,  7.3696e-02,  1.0082e-01, -7.4945e-02, -1.6529e-01,\n",
       "          -1.3616e-01,  2.4045e-01,  3.7273e-01, -8.1969e-02,  2.0698e-01,\n",
       "          -2.3342e-01, -1.0784e-02, -1.8337e-02, -6.2456e-03,  2.2902e-01,\n",
       "          -1.6063e-01, -9.1238e-02,  3.9065e-01,  1.0085e-01, -1.3895e-01,\n",
       "          -2.6798e-01,  1.0546e-01,  2.9551e-01,  1.9659e-01, -1.8986e-01,\n",
       "           1.0085e-01, -8.6333e-04,  2.1887e-01,  2.2593e-01,  5.7697e-02,\n",
       "           2.0147e-01, -8.3542e-03,  3.9365e-01,  1.6220e-01,  2.6417e-01,\n",
       "           4.5439e-02, -1.4156e-01,  2.8990e-02, -9.7075e-02, -5.0514e-01,\n",
       "           1.4804e-01,  2.5942e-01, -3.5281e-01,  4.3952e-02,  2.1822e-01,\n",
       "          -1.6508e-01, -1.0573e-01, -1.6742e-01, -2.3362e-02,  2.3466e-01,\n",
       "          -1.7052e-01,  1.8291e-01, -9.3705e-02, -2.8826e-01,  1.3164e-01,\n",
       "          -6.6831e-02, -3.8133e-02, -4.7518e-02,  2.1711e-01,  9.5190e-02,\n",
       "           1.4235e-01, -2.1930e-01,  1.1346e-01, -2.4436e-01, -4.6103e-02,\n",
       "          -9.0851e-02,  1.5285e-01,  2.6343e-01, -1.8241e-01, -2.0849e-01,\n",
       "           1.7096e-01,  1.7134e-02, -1.3500e-01,  4.2034e-02,  2.4136e-01,\n",
       "           1.1496e-01,  3.4276e-04, -2.4993e-01, -2.5651e-01,  7.0107e-02,\n",
       "           4.7648e-01, -5.9315e-02,  2.2553e-01,  1.0187e-02, -3.4997e-01,\n",
       "          -1.8473e-01,  3.1212e-02, -4.3680e-01,  1.9710e-01,  1.0237e-01,\n",
       "           3.5916e-03,  1.1852e-01, -9.8785e-02, -9.5019e-03, -4.9916e-02,\n",
       "          -9.8211e-02, -1.2774e-01,  2.0071e-01, -1.1970e-01,  3.0694e-01,\n",
       "           1.0382e-01,  2.4789e-01, -2.6453e-01,  1.0652e-01, -8.9998e-02,\n",
       "          -1.4049e-02, -1.8516e-01, -1.2128e-01, -3.4149e-02,  1.2881e-03,\n",
       "          -1.1515e-04, -1.6823e-02,  9.6228e-02,  3.3772e-01, -8.3013e-02,\n",
       "           7.1780e-02,  5.0532e-03, -1.4972e-01,  9.1001e-02, -9.1059e-02,\n",
       "           3.0113e-01, -5.5145e-02, -1.2777e-01, -9.5665e-02, -2.4434e-01,\n",
       "           3.9643e-01,  2.5326e-02,  6.0706e-02, -1.0090e-01,  4.8103e-02,\n",
       "          -7.5343e-03,  1.2491e-01,  3.7102e-01,  2.0583e-01, -1.4503e-02,\n",
       "           5.8739e-02,  1.8199e-01,  2.5642e-02,  2.2271e-01,  2.2445e-01,\n",
       "           4.6583e-01, -1.7752e-01,  4.3370e-01,  7.9620e-03, -7.6244e-02,\n",
       "           7.7768e-02, -3.7950e-01, -8.4357e-02,  1.9338e-01,  6.1943e-03,\n",
       "          -1.3441e-01, -3.8595e-02, -1.7833e-01,  3.0797e-01, -1.4802e-02,\n",
       "           1.4394e-01, -2.3650e-01, -8.2309e-02, -3.3004e-02, -8.9513e-02,\n",
       "           1.0998e-01,  7.1607e-02,  2.4809e-01, -7.1714e-02, -1.7597e-01,\n",
       "          -1.4145e-01, -2.0798e-01, -2.7077e-01, -2.7073e-01,  2.6604e-01,\n",
       "          -7.6303e-02,  3.5127e-01, -2.5082e-01,  1.4383e-01,  3.9476e-02,\n",
       "           1.5016e-01, -5.3990e-02, -3.7349e-01,  1.5657e-01, -4.1016e-01,\n",
       "          -5.8420e-02, -2.2650e-01, -6.4920e-02, -3.1274e-01,  1.1956e-02,\n",
       "          -2.0950e-01, -2.3066e-01, -1.1189e-01, -1.7152e-01,  2.7737e-01,\n",
       "          -2.4200e-01, -1.0078e-01,  1.3553e-01,  3.7450e-01, -2.0825e-01,\n",
       "          -1.3794e-01,  4.3218e-02, -1.3801e-01, -2.2804e-01,  2.7507e-01,\n",
       "          -3.1452e-01, -6.4809e-02, -1.5959e-01,  8.1682e-02, -1.7129e-01,\n",
       "          -7.8972e-02, -6.7335e-03,  1.7910e-01,  3.4509e-01,  7.3647e-03,\n",
       "          -1.4328e-01, -5.0390e-01, -1.0951e-01,  2.0472e-01,  2.5615e-01,\n",
       "           2.3614e-01,  1.5249e-01,  2.6674e-01,  1.5803e-01,  3.7216e-02,\n",
       "           2.3908e-01,  2.2824e-01, -1.4867e-01,  5.2766e-02, -3.1762e-01,\n",
       "          -2.9675e-01, -9.1955e-03,  2.7389e-01, -2.8222e-01,  3.1714e-03]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_document_to_torch_input(document):\n",
    "    inputs = [torch.from_numpy(sentence).view(1,-1) for sentence in document]\n",
    "    return torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "inputs = convert_document_to_torch_input(w2v_averaged[authors[0]][0])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindAuthor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FindAuthor, self).__init__()\n",
    "        self.GRU = nn.GRU(300, 200, bias = False)     \n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size = 1, stride = 2, bias = False)\n",
    "        self.linear = nn.Linear(100,50)\n",
    "        \n",
    "    def forward(self, sentence_input):\n",
    "        out, hidden = self.GRU(sentence_input)\n",
    "        \n",
    "        conv_out = self.conv(out)\n",
    "        pooled_output = torch.mean(conv_out, 0, True)\n",
    "        probs = self.linear(pooled_output)\n",
    "        return F.log_softmax(probs, dim=2).view(1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8145, -3.8774, -3.8813, -3.8179, -3.9542, -3.8930, -3.8291, -3.8602,\n",
       "         -4.0140, -3.8495, -3.8965, -3.9199, -3.9147, -4.0094, -3.8922, -3.8874,\n",
       "         -3.7946, -3.9366, -3.9904, -3.9628, -3.9095, -3.9512, -3.8857, -3.9454,\n",
       "         -3.8606, -3.8871, -3.8994, -4.0077, -3.9541, -3.8839, -3.9279, -3.9502,\n",
       "         -3.8718, -3.9030, -3.8406, -3.9147, -3.9262, -3.9685, -3.9843, -3.9457,\n",
       "         -3.9211, -3.9020, -3.9931, -3.9740, -4.0628, -3.9456, -3.9573, -3.8247,\n",
       "         -3.8648, -3.8276]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FindAuthor()\n",
    "net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([x for x in range(50)])\n",
    "targets = torch.from_numpy(np.repeat(x, [90 for y in range(50)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for author in authors:\n",
    "    for document in w2v_averaged[author]:\n",
    "        training_data.append(convert_document_to_torch_input(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumbled_training_data = []\n",
    "for i in range(len(w2v_averaged[author])):\n",
    "    for author in authors:\n",
    "        jumbled_training_data.append(convert_document_to_torch_input(w2v_averaged[author][i]))\n",
    "jumbled_targets = torch.tensor([x for x in range(50)]*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2431, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7472, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5412, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4142, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3519, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3100, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2576, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1830, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1006, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0707, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1119, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0396, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1250, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1361, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0767, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1193, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0062, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0234, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0077, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0029, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# mini-batch gradient descent\n",
    "torch.manual_seed(0)\n",
    "model = FindAuthor()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "\n",
    "for epoch in range(300):  \n",
    "    tags = []\n",
    "    for i in range(len(jumbled_training_data)):\n",
    "        model.zero_grad()\n",
    "        document = jumbled_training_data[i]\n",
    "        tags.append(model(document))\n",
    "        if i % 50 == 49:\n",
    "            tag_scores = torch.cat(tags)\n",
    "            loss = loss_function(tag_scores, jumbled_targets[:50])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tags = []\n",
    "    print(loss)\n",
    "    if loss < 0.001:\n",
    "        print(epoch)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.53333333333333%\n"
     ]
    }
   ],
   "source": [
    "training_predictions = [np.argmax(model(document).view(-1).detach().numpy()) for document in training_data]\n",
    "print(\"Training Accuracy: {}%\".format(100*np.mean(targets.detach().numpy() == training_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_averaged_test = {}\n",
    "for author in authors:\n",
    "    w2v_averaged_test[author] = []\n",
    "    for document in test_processed_2[author]:\n",
    "        w2v_document_averaged = []\n",
    "        for sentence in document:\n",
    "            w2v_sentence = [glove_vectors[word] for word in sentence if word in glove_vectors and word in vocab]\n",
    "            if len(w2v_sentence) == 0:\n",
    "                continue\n",
    "            weights = [idf[word] for word in sentence if word in glove_vectors and word in vocab]\n",
    "            if sum(weights) != 0:\n",
    "                w2v_sentence_averaged = np.zeros(len(w2v_sentence[0]))\n",
    "                for i in range(len(weights)):\n",
    "                    w2v_sentence_averaged += w2v_sentence[i]*weights[i]\n",
    "                w2v_sentence_averaged /= sum(weights)\n",
    "                w2v_sentence_averaged = w2v_sentence_averaged.astype(np.float32)\n",
    "            else:\n",
    "                w2v_sentence_averaged = np.mean(np.array(w2v_sentence), axis=0)\n",
    "            w2v_document_averaged.append(w2v_sentence_averaged)\n",
    "        w2v_averaged_test[author].append(w2v_document_averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = []\n",
    "for author in authors:\n",
    "    for document in w2v_averaged_test[author]:\n",
    "        testing_data.append(convert_document_to_torch_input(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 78.0%\n"
     ]
    }
   ],
   "source": [
    "testing_predictions = [np.argmax(model(document).view(-1).detach().numpy()) for document in testing_data]\n",
    "print(\"Testing Accuracy: {}%\".format(100*np.mean(np.repeat(x, [10 for y in range(50)], axis=0) == testing_predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
